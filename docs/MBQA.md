# Multi-task Binary Question Answering (MBQA)

## Overview
The Multi-task Binary Question Answering (MBQA) module is designed to train a model that can answer multiple binary (yes/no) questions about a given text simultaneously. It uses the questions generated by the Contrastive Question Generation (CQG) module to create a large-scale training dataset automatically and then trains a multi-task classifier.

## Usage
The MBQA module can be used as follows:
```python
from mbqa import MBQA

mbqa = MBQA(
        corpus=doc_texts,
        temp_folder="./temp",
        output_folder="./output",
        name="DatasetName"
    )
mbqa.collect_training_data()
mbqa.train_model()
```
Arguments for the MBQA class:

- `corpus`: (Required) List of text documents to use for training. 
> ❗ The `corpus` must be the same as the one used for the CQG module.

- `temp_folder`: (Default: "./temp") Directory for storing temporary files during processing.
> ❗ This folder must be the same as the one used for the CQG module.

- `name`: (Required) Name for the current run, used for organizing temporary and output files.
> ❗ This name must be the same as the one used for the CQG module.

- `output_folder`: (Default: "./output") Directory for storing the final output files.

- `LLM`: (Default: "gpt-4o-mini") Language model used for generating answers to the binary questions.

- `backbone`: (Default: "WhereIsAI/UAE-Large-V1") The backbone model used for the multi-task classifier.

- `learning_rate`: (Default: 1e-4) Learning rate for the optimizer during model training.

- `num_steps`: (Default: 3000000) Number of training steps for the model.

- `openai_api_key`: (Default: None) OpenAI API key. If not provided, it will attempt to use the API key saved in the environment variable `OPENAI_API_KEY`.

- `device`: (Default: "cuda") Device to use for training ("cuda" for GPU, "cpu" for CPU).

The MBQA process consists of two main steps:

1. `collect_training_data()`: This function generates a large-scale training dataset by using the questions generated by the CQG module and obtaining answers for these questions on a subset of the corpus. We collect 1000 articles for each question by default, and this is sufficient empirically for training a high-quality MBQA model.

2. `train_model()`: This function trains a multi-task classifier using the collected training data. The classifier learns to answer multiple binary questions about a given text simultaneously.

> ⚠️ The `collect_training_data()` process may take a considerable amount of time, depending on the the number of questions. It involves making multiple API calls to the specified LLM.

> 💡 The `train_model()` function will train the model for the specified number of steps. You can adjust the `num_steps` parameter based on your dataset size and desired training time.

After training, pipeline produce two files:
- `questions.json`: A JSON file containing the generated questions. This is needed for producing result interpretation when using the model for inference.
- `mbqa_model.pt`: A PyTorch model checkpoint. This is needed for loading the model for inference.